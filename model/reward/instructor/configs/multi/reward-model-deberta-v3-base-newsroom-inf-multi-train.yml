model_name: OpenAssistant/reward-model-deberta-v3-base
learning_rate: 5e-5
scheduler: constant
gradient_checkpointing: false
gradient_accumulation_steps: 6
per_device_train_batch_size: 1
per_device_eval_batch_size: 4
warmup_steps: 0
eval_steps: 50
save_steps: 50
max_length: 4096
num_train_epochs: 3
auto_find_batch_size: false
report_to:
  - tensorboard
summeval_path: /root/datasets/sensitivity_equal_chunks/RelevanceRating_dataset/
output_dirs:
  - ./models/rel/base/train0/
  - ./models/rel/base/train1/
  - ./models/rel/base/train2/
  - ./models/rel/base/train3/
  - ./models/rel/base/train4/
  - ./models/rel/base/train5/
  - ./models/rel/base/train6/
  - ./models/rel/base/train7/
  - ./models/rel/base/train8/
  - ./models/rel/base/train9/
train_splits:
  - [train_0]
  - [train_1]
  - [train_2]
  - [train_3]
  - [train_4]
  - [train_5]
  - [train_6]
  - [train_7]
  - [train_8]
  - [train_9]
datasets:
  - newsroom_local
