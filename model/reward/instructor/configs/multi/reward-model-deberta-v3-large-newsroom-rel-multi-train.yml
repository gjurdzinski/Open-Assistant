model_name: OpenAssistant/reward-model-deberta-v3-large
learning_rate: 5e-5
scheduler: constant
gradient_checkpointing: true
gradient_accumulation_steps: 6
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
warmup_steps: 0
eval_steps: 50
save_steps: 50
max_length: 4096
num_train_epochs: 3
auto_find_batch_size: false
report_to:
  - tensorboard
summeval_path: /root/datasets/sensitivity_equal_chunks/RelevanceRating_dataset/
output_dirs:
  # - ./models/rel/large/train0/
  # - ./models/rel/large/train1/
  # - ./models/rel/large/train2/
  # - ./models/rel/large/train3/
  # - ./models/rel/large/train4/
  # - ./models/rel/large/train5/
  # - ./models/rel/large/train6/
  # - ./models/rel/large/train7/
  # - ./models/rel/large/train8/
  - ./models/rel/large/train9/
train_splits:
  # - [train_0]
  # - [train_1]
  # - [train_2]
  # - [train_3]
  # - [train_4]
  # - [train_5]
  # - [train_6]
  # - [train_7]
  # - [train_8]
  - [train_9]
datasets:
  - newsroom_local
